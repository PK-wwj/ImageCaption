{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import requests\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/kaggle.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1dacf2a62425>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# 将API凭据保存到kaggle.json文件中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/kaggle.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/kaggle.json'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/content/data/'):\n",
    "\n",
    "    api_token = {\"username\": \"<-- your username -->\",\n",
    "                 \"key\": \"<-- your api key -->\"}\n",
    "\n",
    "    with open('/content/kaggle.json', 'w') as file:\n",
    "        json.dump(api_token, file)\n",
    "\n",
    "    os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/\"\n",
    "\n",
    "    os.system('kaggle datasets download -d adityajn105/flickr8k')\n",
    "    os.makedirs('/content/data/', exist_ok=True)\n",
    "    os.system('mv /content/flickr8k.zip /content/data/flickr8k.zip')\n",
    "    os.system('unzip -q /content/data/flickr8k.zip -d /content/data/')\n",
    "    os.remove('/content/data/flickr8k.zip')\n",
    "'''在kaggle上下载flickr8k数据集，8000张图，建议只挑很小的一部分来训练'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 从 '/content/data/captions.txt' 文件中读取字幕数据\n",
    "captions = pd.read_csv('/content/data/captions.txt')\n",
    "\n",
    "# 将 'image' 列的值进行处理，拼接路径 '/content/data/Images/' 和文件名\n",
    "captions['image'] = captions['image'].apply(lambda x: f'/content/data/Images/{x}')\n",
    "\n",
    "# 打印前几行数据\n",
    "captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    # 将文本转换为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 去除非字母和非空格字符\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # 将连续的空格替换为单个空格\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # 去除文本两端的空格\n",
    "    text = text.strip()\n",
    "    \n",
    "    # 在文本前后添加特殊标记\n",
    "    text = '[start] ' + text + ' [end]'\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'captions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-80a774c7cc6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'caption'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'caption'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 注释：\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 使用 apply() 方法将 preprocess 函数应用到 captions['caption'] 列的每个元素上，对每个字幕进行预处理。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'captions' is not defined"
     ]
    }
   ],
   "source": [
    "captions['caption'] = captions['caption'].apply(preprocess)\n",
    "captions.head()\n",
    "# 注释：\n",
    "\n",
    "# 使用 apply() 方法将 preprocess 函数应用到 captions['caption'] 列的每个元素上，对每个字幕进行预处理。\n",
    "# 将预处理后的字幕存储回 captions['caption'] 列。\n",
    "# 使用 head() 方法显示前几行的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_row = captions.sample(1).iloc[0]\n",
    "print(random_row.caption)\n",
    "print()\n",
    "im = Image.open(random_row.image)\n",
    "\n",
    "# 注释：\n",
    "\n",
    "# 使用 sample() 方法从 captions 数据框中随机抽取一行数据，并通过 iloc[0] 获取抽取行的内容。\n",
    "# 打印随机抽取的字幕内容。\n",
    "# 创建 Image 对象，使用 Image.open() 方法打开对应的图像文件。\n",
    "# 显示图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MAX_LENGTH：定义生成的字幕的最大长度，超过该长度的字幕将被截断或省略。\\nVOCABULARY_SIZE：定义字幕词汇表的大小，即词汇表中不同单词的数量。\\nBATCH_SIZE：定义用于训练的每个批次的样本数量。\\nBUFFER_SIZE：定义用于数据集缓冲的大小，用于混洗和预取数据。\\nEMBEDDING_DIM：定义嵌入层的维度，用于将单词编码为密集向量表示。\\nUNITS：定义LSTM或Transformer模型中隐藏层的单元数量。这决定了模型的复杂度和记忆能力。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 40\n",
    "VOCABULARY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "EMBEDDING_DIM = 512\n",
    "UNITS = 512\n",
    "'''MAX_LENGTH：定义生成的字幕的最大长度，超过该长度的字幕将被截断或省略。\n",
    "VOCABULARY_SIZE：定义字幕词汇表的大小，即词汇表中不同单词的数量。\n",
    "BATCH_SIZE：定义用于训练的每个批次的样本数量。\n",
    "BUFFER_SIZE：定义用于数据集缓冲的大小，用于混洗和预取数据。\n",
    "EMBEDDING_DIM：定义嵌入层的维度，用于将单词编码为密集向量表示。\n",
    "UNITS：定义LSTM或Transformer模型中隐藏层的单元数量。这决定了模型的复杂度和记忆能力。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9e2de2ae78d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 创建文本标记器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m tokenizer = tf.keras.layers.TextVectorization(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVOCABULARY_SIZE\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# 定义词汇表大小\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstandardize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# 不应用标准化函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moutput_sequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m  \u001b[1;31m# 定义输出序列长度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# 创建文本标记器\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE,  # 定义词汇表大小\n",
    "    standardize=None,  # 不应用标准化函数\n",
    "    output_sequence_length=MAX_LENGTH  # 定义输出序列长度\n",
    ")\n",
    "\n",
    "# 适应标记器到字幕数据\n",
    "tokenizer.adapt(captions['caption'])  # 将标记器适应到字幕数据中的'caption'列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3858887f1294>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m word2idx = tf.keras.layers.StringLookup(\n\u001b[0;32m      3\u001b[0m     \u001b[0mmask_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# 设置掩码标记为空\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 使用之前创建的tokenizer的词汇表作为单词列表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 创建StringLookup层用于单词到索引的映射\n",
    "word2idx = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",  # 设置掩码标记为空\n",
    "    vocabulary=tokenizer.get_vocabulary()  # 使用之前创建的tokenizer的词汇表作为单词列表\n",
    ")\n",
    "\n",
    "# 创建StringLookup层用于索引到单词的映射\n",
    "idx2word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",  # 设置掩码标记为空\n",
    "    vocabulary=tokenizer.get_vocabulary(),  # 使用之前创建的tokenizer的词汇表作为单词列表\n",
    "    invert=True  # 反转映射，从索引到单词\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个字典，用于存储每个图像对应的多个标题\n",
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "\n",
    "# 将图像和标题一一对应，存入字典中\n",
    "for img, cap in zip(captions['image'], captions['caption']):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# 获取图像的键列表并进行随机打乱\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "# 将图像键列表切分为训练集和验证集\n",
    "slice_index = int(len(img_keys) * 0.8)\n",
    "img_name_train_keys, img_name_val_keys = (img_keys[:slice_index], \n",
    "                                          img_keys[slice_index:])\n",
    "\n",
    "# 构建训练集的图像和标题列表\n",
    "train_imgs = []\n",
    "train_captions = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    train_imgs.extend([imgt] * capt_len)\n",
    "    train_captions.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "# 构建验证集的图像和标题列表\n",
    "val_imgs = []\n",
    "val_captions = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    val_imgs.extend([imgv] * capv_len)\n",
    "    val_captions.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_path, caption):\n",
    "    # 从文件路径中读取图像数据\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 解码图像数据为张量\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    # 调整图像大小为指定尺寸\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    # 归一化图像数据\n",
    "    img = img / 255.\n",
    "    # 使用预先定义的tokenizer对标题进行编码\n",
    "    caption = tokenizer(caption)\n",
    "    return img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_imgs, train_captions))\n",
    "\n",
    "# 使用map函数对训练数据集进行预处理，同时设置并行调用的数量\n",
    "train_dataset = train_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# 打乱数据并设置批次大小\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_imgs, val_captions))\n",
    "\n",
    "# 使用map函数对验证数据集进行预处理，同时设置并行调用的数量\n",
    "val_dataset = val_dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# 打乱数据并设置批次大小\n",
    "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "这段代码定义了一个图像增强的序列模型，其中包含三个图像增强的层：\n",
    "\n",
    "tf.keras.layers.RandomFlip(\"horizontal\")：随机水平翻转图像。\n",
    "tf.keras.layers.RandomRotation(0.2)：随机旋转图像，旋转角度在-0.2到0.2之间。\n",
    "tf.keras.layers.RandomContrast(0.3)：随机调整图像对比度，对比度系数在0.7到1.3之间。\n",
    "你可以将这个图像增强的序列模型应用于训练数据集的图像上，以增加数据的多样性和鲁棒性。\n",
    "例如，可以在load_data函数中的图像处理部分之前使用该模型，将图像增强应用到每个训练样本上。\n",
    "这样可以在训练过程中随机改变图像的水平翻转、旋转和对比度，从而增加模型对不同变体的图像的泛化能力。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Encoder():\n",
    "    # 使用预训练的InceptionV3模型作为基础\n",
    "    inception_v3 = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    # 冻结InceptionV3模型的权重，不参与训练\n",
    "    inception_v3.trainable = False\n",
    "\n",
    "    # 获取InceptionV3模型的输出层\n",
    "    output = inception_v3.output\n",
    "    # 重塑输出，转换为2D张量\n",
    "    output = tf.keras.layers.Reshape(\n",
    "        (-1, output.shape[-1]))(output)\n",
    "\n",
    "    # 创建CNN编码器模型，输入为InceptionV3模型的输入层，输出为重塑后的特征张量\n",
    "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个CNN编码器模型，使用预训练的InceptionV3模型作为基础。\n",
    "\n",
    "首先，通过tf.keras.applications.InceptionV3函数创建了一个InceptionV3模型，设置include_top=False表示不包括顶部的全连接层，weights='imagenet'表示加载在ImageNet上预训练的权重。\n",
    "\n",
    "接下来，将InceptionV3模型的输出层作为输入，并通过tf.keras.layers.Reshape层将输出进行重塑，将其转换为形状为(-1, output.shape[-1])的张量。这样做是为了将输出转换为2D张量，便于后续的处理。\n",
    "\n",
    "最后，通过tf.keras.models.Model函数创建一个模型，将InceptionV3模型的输入层和重塑后的输出层作为输入和输出，得到最终的CNN编码器模型。\n",
    "\n",
    "这个CNN编码器模型将输入的图像通过预训练的InceptionV3模型进行特征提取，并将提取到的特征转换为2D张量作为输出。这些特征将用作后续的图像字幕生成模型的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
    "    \n",
    "    def call(self, x, training):\n",
    "        # 应用 Layer Normalization\n",
    "        x = self.layer_norm_1(x)\n",
    "        # 进行全连接层操作\n",
    "        x = self.dense(x)\n",
    "\n",
    "        # 进行多头自注意力机制\n",
    "        attn_output = self.attention(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        # 跳连接和残差连接\n",
    "        x = self.layer_norm_2(x + attn_output)\n",
    "        return x\n",
    "    '''这段代码定义了Transformer模型的一个编码器层。在__init__方法中，\n",
    "    使用tf.keras.layers.LayerNormalization定义了两个Layer Normalization层，\n",
    "    分别用于在自注意力机制前后进行归一化操作。使用tf.keras.layers.MultiHeadAttention定义了一个多头注意力层，\n",
    "    其中num_heads表示头的数量，key_dim表示注意力机制中键的维度。使用tf.keras.layers.Dense定义了一个全连接层，\n",
    "    将输入的维度转换为embed_dim，并使用ReLU作为激活函数。\n",
    "\n",
    "在call方法中，首先对输入应用Layer Normalization进行归一化操作，\n",
    "然后通过全连接层进行维度转换。接下来，调用多头注意力层，传入输入的查询、键和值，并指定注意力屏蔽和训练模式。\n",
    "注意力层会计算注意力权重，并将其应用于值，得到注意力输出。最后，将注意力输出与跳连接的输入相加，并再次应用Layer Normalization\n",
    "，得到编码器层的输出。这个编码器层可以被堆叠多次以构建更深的Transformer编码器。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            vocab_size, embed_dim)\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            max_len, embed_dim, input_shape=(None, max_len))\n",
    "    \n",
    "    def call(self, input_ids):\n",
    "        # 获取输入序列的长度\n",
    "        length = tf.shape(input_ids)[-1]\n",
    "        # 生成位置编码的索引\n",
    "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
    "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
    "\n",
    "        # 获取词嵌入和位置嵌入\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        # 将词嵌入和位置嵌入相加得到最终的嵌入向量\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这段代码定义了Transformer模型的嵌入层。在__init__方法中，使用tf.keras.layers.Embedding定义了两个嵌入层，分别用于词嵌入和位置嵌入。vocab_size表示词汇表的大小，embed_dim表示嵌入的维度，max_len表示输入序列的最大长度。\n",
    "\n",
    "在call方法中，首先通过tf.shape获取输入序列的长度，然后使用tf.range生成位置编码的索引。将位置编码的索引进行扩展，使其与输入序列的维度相匹配。\n",
    "\n",
    "接下来，通过词嵌入层将输入序列转换为词嵌入向量，通过位置嵌入层将位置编码转换为位置嵌入向量。\n",
    "\n",
    "最后，将词嵌入向量和位置嵌入向量相加得到最终的嵌入向量。这个嵌入向量将作为输入传递给Transformer的编码器层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings(tokenizer.vocabulary_size(), EMBEDDING_DIM, MAX_LENGTH)(next(iter(train_dataset))[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码计算了使用Embeddings类进行嵌入操作后的输出形状。首先创建了Embeddings对象，传入了词汇表大小、嵌入维度和最大序列长度作为参数。\n",
    "\n",
    "然后通过next(iter(train_dataset))[1]获取训练数据集中的一个批次的输入序列，并将其作为参数传递给嵌入层的call方法。得到的输出被赋值给embedding_output。\n",
    "\n",
    "最后打印出embedding_output的形状，即嵌入后的向量形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, units, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = Embeddings(\n",
    "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
    "\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "    \n",
    "\n",
    "    def call(self, input_ids, encoder_output, training, mask=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "\n",
    "        combined_mask = None\n",
    "        padding_mask = None\n",
    "        \n",
    "        if mask is not None:\n",
    "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attn_output_1 = self.attention_1(\n",
    "            query=embeddings,\n",
    "            value=embeddings,\n",
    "            key=embeddings,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
    "\n",
    "        attn_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_output,\n",
    "            key=encoder_output,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段代码定义了一个Transformer解码器层的自定义Keras层，包括了嵌入层、多头注意力机制、层归一化、前馈神经网络等组件。\n",
    "\n",
    "在call方法中，首先将输入序列input_ids通过嵌入层self.embedding进行嵌入操作。然后根据是否存在掩码mask，构造综合注意力掩码和填充掩码。利用多头注意力机制self.attention_1和self.attention_2分别进行自注意力和编码器-解码器注意力的计算。\n",
    "\n",
    "接着通过层归一化和残差连接将注意力输出与嵌入层的输出进行结合，并经过前馈神经网络和层归一化进行进一步的处理。\n",
    "\n",
    "最后，使用self.out进行分类预测，并返回预测结果。\n",
    "\n",
    "get_causal_attention_mask方法用于生成自回归式的注意力掩码，保证解码器在生成序列时只能依赖于已生成的部分，不会使用后续的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_aug = image_aug\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "    \n",
    "\n",
    "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
    "        encoder_output = self.encoder(img_embed, training=True)\n",
    "        y_input = captions[:, :-1]\n",
    "        y_true = captions[:, 1:]\n",
    "        mask = (y_true != 0)\n",
    "        y_pred = self.decoder(\n",
    "            y_input, encoder_output, training=True, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
    "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        if self.image_aug:\n",
    "            imgs = self.image_aug(imgs)\n",
    "        \n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = self.compute_loss_and_acc(\n",
    "                img_embed, captions\n",
    "            )\n",
    "    \n",
    "        train_vars = (\n",
    "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        )\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "    \n",
    "\n",
    "    def test_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        loss, acc = self.compute_loss_and_acc(\n",
    "            img_embed, captions, training=False\n",
    "        )\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个用于图像字幕生成的模型类ImageCaptioningModel，继承自tf.keras.Model。\n",
    "\n",
    "在__init__方法中，初始化了CNN模型cnn_model、编码器encoder、解码器decoder和图像增强器image_aug。同时，定义了用于跟踪损失和准确率的loss_tracker和acc_tracker。\n",
    "\n",
    "calculate_loss方法计算损失函数，根据预测值y_pred和真实值y_true以及掩码mask计算交叉熵损失，并根据掩码对损失进行加权。\n",
    "\n",
    "calculate_accuracy方法计算准确率，根据预测值y_pred和真实值y_true以及掩码mask计算序列准确率，并根据掩码对准确率进行加权。\n",
    "\n",
    "compute_loss_and_acc方法用于计算损失和准确率。首先通过编码器self.encoder对图像进行编码得到特征向量encoder_output，然后根据输入序列captions预测下一个词，并计算损失和准确率。\n",
    "\n",
    "train_step方法定义了每个训练步骤的操作。首先对图像进行图像增强处理（如果有的话），然后通过CNN模型self.cnn_model对图像进行特征提取得到图像嵌入img_embed。接着使用自定义的compute_loss_and_acc方法计算损失和准确率，并使用梯度带计算梯度并更新模型参数。最后更新损失和准确率的跟踪器。\n",
    "\n",
    "test_step方法定义了每个测试步骤的操作。通过CNN模型对图像进行特征提取得到图像嵌入img_embed，然后使用compute_loss_and_acc方法计算损失和准确率。最后更新损失和准确率的跟踪器。\n",
    "\n",
    "metrics属性返回模型的度量指标，包括损失和准确率的跟踪器。\n",
    "\n",
    "这个模型类可以用于训练和评估图像字幕生成模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)  # 创建Transformer编码器层\n",
    "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)  # 创建Transformer解码器层\n",
    "\n",
    "cnn_model = CNN_Encoder()  # 创建CNN模型\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
    ")  # 创建图像字幕生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这段代码中，首先创建了Transformer编码器层encoder和Transformer解码器层decoder，使用了预定义的EMBEDDING_DIM和UNITS参数。\n",
    "\n",
    "然后，创建了CNN模型cnn_model，使用了预定义的CNN_Encoder函数。\n",
    "\n",
    "最后，通过ImageCaptioningModel类创建了图像字幕生成模型caption_model，将CNN模型、编码器和解码器作为参数传递进去，并指定了图像增强器image_augmentation。\n",
    "\n",
    "这个模型可以用于训练和生成图像字幕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")  # 使用稀疏分类交叉熵作为损失函数，不计算logits，返回未经缩减的损失\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, restore_best_weights=True\n",
    ")  # 提前停止训练的回调函数，如果连续3个epoch没有改善，恢复到最佳权重状态\n",
    "\n",
    "caption_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),  # 使用Adam优化器\n",
    "    loss=cross_entropy  # 损失函数使用交叉熵\n",
    ")  # 编译图像字幕生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这段代码中，首先创建了稀疏分类交叉熵损失函数cross_entropy，其中from_logits=False表示输入的预测结果经过了softmax，reduction=\"none\"表示不进行损失的缩减。\n",
    "\n",
    "然后，创建了EarlyStopping回调函数early_stopping，设置了patience=3表示连续3个epoch没有改善时停止训练，并使用restore_best_weights=True恢复到最佳权重状态。\n",
    "\n",
    "最后，通过compile方法，配置了图像字幕生成模型caption_model的优化器为Adam优化器，并将损失函数设置为前面定义的交叉熵损失函数cross_entropy。\n",
    "\n",
    "这样，模型就准备好进行训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = caption_model.fit(\n",
    "    train_dataset,  # 训练数据集\n",
    "    epochs=5,  # 迭代次数\n",
    "    validation_data=val_dataset,  # 验证数据集\n",
    "    callbacks=[early_stopping]  # 回调函数，提前停止训练\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调用fit方法，开始对图像字幕生成模型进行训练。传入训练数据集train_dataset作为训练数据，设置迭代次数为5次epochs=5，传入验证数据集val_dataset作为验证数据用于评估模型性能。还传入了提前停止训练的回调函数callbacks=[early_stopping]，以便在连续3个epoch没有改善时提前停止训练，并恢复到最佳权重状态。\n",
    "\n",
    "训练过程中的训练损失、验证损失以及其他指标可以通过history对象来访问，如history.history['loss']可以获取训练过程中的训练损失历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word(2).numpy().decode('utf-8')\n",
    "#要使用idx2word进行索引查找，您需要传递一个整数索引作为输入，并使用.numpy()方法将结果转换为NumPy数组。\n",
    "#然后，您可以使用.decode('utf-8')将NumPy数组转换为UTF-8编码的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_path(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = img / 255.\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_caption(img_path):\n",
    "    img = load_image_from_path(img_path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img_embed = caption_model.cnn_model(img)\n",
    "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
    "\n",
    "    y_inp = '[start]'\n",
    "    for i in range(MAX_LENGTH-1):\n",
    "        tokenized = tokenizer([y_inp])[:, :-1]\n",
    "        mask = tf.cast(tokenized != 0, tf.int32)\n",
    "        pred = caption_model.decoder(\n",
    "            tokenized, img_encoded, training=False, mask=mask)\n",
    "        \n",
    "        pred_idx = np.argmax(pred[0, i, :])\n",
    "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
    "        if pred_word == '[end]':\n",
    "            break\n",
    "        \n",
    "        y_inp += ' ' + pred_word\n",
    "    \n",
    "    y_inp = y_inp.replace('[start] ', '')\n",
    "    return y_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "上面的代码定义了两个函数：\n",
    "\n",
    "load_image_from_path(img_path): 从给定的图像路径加载图像，并进行预处理和归一化处理。该函数返回处理后的图像张量。\n",
    "\n",
    "generate_caption(img_path): 生成图像的标题。首先，它会加载和预处理图像。然后，它使用caption_model中的CNN模型和Transformer模型编码图像，并根据已生成的部分标题预测下一个词。循环迭代直到生成的标题达到最大长度或预测的词为[end]。最后，返回生成的标题字符串。\n",
    "\n",
    "您可以将所需的图像路径传递给generate_caption函数，它将返回生成的标题字符串。请确保您已经训练并加载了适当的模型，并且caption_model是已经实例化的ImageCaptioningModel模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从验证集中随机选择一张图像\n",
    "idx = random.randrange(0, len(val_imgs))\n",
    "img_path = val_imgs[idx]\n",
    "\n",
    "# 使用图像路径生成预测的标题\n",
    "pred_caption = generate_caption(img_path)\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "\n",
    "# 显示图像和生成的标题\n",
    "image = Image.open(img_path)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# 下载图像并保存为临时文件\n",
    "url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT2j6yclbKYDav4BGUKLAdTvSFXp1gtuzy5DQ&usqp=CAU\"\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "im.save('tmp.jpg')\n",
    "\n",
    "# 生成图像的标题\n",
    "pred_caption = generate_caption('tmp.jpg')\n",
    "\n",
    "# 打印生成的标题并显示图像\n",
    "print('Predicted Caption:', pred_caption)\n",
    "#print(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
